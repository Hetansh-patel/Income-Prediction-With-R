---
title: "R Notebook"
output: html_notebook
---

Importing Libraries

```{r}
options(scipen = 99)
library(ggplot2)
library(GGally)
library(rsample)    
library(caTools)
library(reshape2)
library(ggpubr)
library(corrplot)
library(psych)
library(ggpubr)
library(ggcorrplot)
library(pROC)
library(caret)
library(rpart)        
library(rpart.plot)
library(plotrix)
library(e1071)
library(randomForest)
```
Creating new dataset

```{r}
df = adult
```

summarizing data what we are working with
```{r}
summary(df)
```
checking if any column is NA
```{r}
colSums(is.na(df))
```

There's a problem in our data. Na values are marked as "?". So we have to fix that first by converting "?" to NA values

```{r}
df[df == "?"] = NA
```

Now its showing our NA values

```{r}
colSums(is.na(df))
```

Every column that has NA values is char type. So even if we fill that values it would be miss leading rather than its better to delete that rows.

```{r}
df = na.omit(df)
```

```{r}
summary(df)
```
Plotting correlation for numeric columns 
```{r}
correlation_plot(df)
```

Checking unique values to make it categorical
```{r}
cat("Unique values in Workclass:\n\n")
unique(df$workclass)
cat('\n')

cat("Unique values in Education:\n\n")
unique(df$education)
cat('\n')

cat("Unique values in Marital Status:\n\n")
unique(df$marital.status)
cat('\n')

cat("Unique values in Occupation:\n\n")
unique(df$occupation)
cat('\n')

cat("Unique values in Relationship Status:\n\n")
unique(df$relationship)
cat('\n')

cat("Unique values in Race:\n\n")
unique(df$race)
cat('\n')

cat("Unique values in Native Country:\n\n")
unique(df$native.country)
cat('\n')

cat("Unique values in Income:\n")
unique(df$income)
```

Converting every possible columns to categorical data such as:

Workclass:

Private = 1
State-gov,Federal-gov,Local-gov = 2
Self_employed = 3
without-pay = 4

Education:

Hs-grad,Bachelors,prof-school = 1
MAsters = 2
Doctorate = 3
Rest = 4

Marital Status:

widowed, Divorced, Never-married, separated = 1 (not married)
married-civ-spouse, married-AF-spouse,Married-spouse-absent = 2 (married)

Relationship status:

Deleting this columns

Race:

White = 1
black = 2
Asian = 3
other,amer-indian-eskimo = 4

Native Country:

united States,canada,mexico,Trinad&Tobago,South, Puerto-rico,Hondurras, Cuba,Nicaragua, Dominican-Republic,Guatemala,EL-Salvador,Jamaica, Ecuador, Outlying-Us, Columbia, Ecuador,    = 1 (America)

Greece, Holad-netherlands, Poland, England, Germany, Italy,Ireland, Hungary, France, Yugoslavia, Portugal, Scotland =2 (Europe)

Vietnam,China, India, Taiwan, Philippines,Iran, Japan, Hong,Cambodia,Laos, Thailand = 3 (asia)

Sex:
Male = 1 
Female = 2

Income:

# <=50K = 1 (less than 50k)
# >50K = 2 (more than 50 K)

```{r}
# Replace 'workclass' values in the existing dataframe
df$workclass <- ifelse(df$workclass == "Private", 1,
                       ifelse(df$workclass %in% c("State-gov", "Federal-gov", "Local-gov"), 2,
                       ifelse(df$workclass == "Without-pay", 4, 3)))

df$education <- ifelse(df$education %in% c("HS-grad", "Bachelors", "Prof-school"), 1,
                             ifelse(df$education == "Masters", 2,
                             ifelse(df$education == "Doctorate", 3, 4)))

df$marital.status <- ifelse(df$marital.status %in% c("Widowed", "Divorced", "Never-married", "Separated"), 1,
                                  ifelse(df$marital.status %in% c("Married-civ-spouse", "Married-AF-spouse", "Married-spouse-absent"), 2, NA))

df$relationship = NULL

df$race <- ifelse(df$race == "White", 1,
                      ifelse(df$race == "Black", 2,
                      ifelse(df$race %in% c("Asian-Pac-Islander", "Amer-Indian-Eskimo"), 3, 4)))

df$native.country <- ifelse(df$native.country %in% c(
    "United-States", "Canada", "Mexico", "Trinadad&Tobago", "South", "Puerto-Rico", "Honduras", "Cuba", "Nicaragua", "Dominican-Republic",
    "Guatemala", "El-Salvador", "Jamaica", "Ecuador", "Outlying-US(Guam-USVI-etc)", "Columbia", "Ecuador","Peru","Haiti"), 1,
  
  ifelse(df$native.country %in% c(
    "Greece", "Holand-Netherlands", "Poland", "England", "Germany", "Italy", "Ireland", "Hungary", "France", "Yugoslavia", "Portugal", "Scotland"), 2,
  
  ifelse(df$native.country %in% c(
    "Vietnam", "China", "India", "Taiwan", "Philippines", "Iran", "Japan", "Hong", "Cambodia", "Laos", "Thailand"), 3, NA)))


df$sex = ifelse(df$sex == "Male", 1,2)


df$income <- ifelse(df$income == "<=50K", 1, 
                           ifelse(df$income == ">50K", 2, NA))



```

```{r}
hours_breaks <- c(1, 40, 45,Inf)
hours_labels <- c(1, 2, 3)

# Create a new factor variable for age groups
df$hours.per.week <- cut(df$hours.per.week, breaks = hours_breaks, labels = hours_labels)


age_breaks <- c(17, 28, 37, 47,Inf)
age_labels <- c(1, 2, 3, 4)

# Create a new factor variable for age groups
df$age <- cut(df$age, breaks = age_breaks, labels = age_labels)

educationnum_breaks <- c(1, 9, 10, 13,Inf)
educationnum_labels <- c(1, 2, 3, 4)

# Create a new factor variable for age groups
df$education.num <- cut(df$education.num, breaks = educationnum_breaks, labels = educationnum_labels)



fnlwgt_breaks <- c(13769, 117627, 178425, 237628,Inf)
fnlwgt_labels <- c(1, 2, 3, 4)

# Create a new factor variable for age groups
df$fnlwgt <- cut(df$fnlwgt, breaks = fnlwgt_breaks, labels = fnlwgt_labels)



```

Creating New dataset from previous data. If we make any mistake in new data we can go back.
```{r}
df1 = df
```

Splitting Occupation Columns to each categories it has and creating new columns in our dataset df1.
```{r}
df1$Exec_managerial <- as.factor(df$occupation == "Exec-managerial")
df1$Prof_specialty <- as.factor(df$occupation == "Prof-specialty")
df1$Machine_op_inspct <- as.factor(df$occupation == "Machine-op-inspct")
df1$Tech_support <- as.factor(df$occupation == "Tech-support")
df1$Craft_repair <- as.factor(df$occupation == "Craft-repair")
df1$Transport_moving <- as.factor(df$occupation == "Transport-moving")
df1$Other_service <- as.factor(df$occupation == "Other-service")
df1$Adm_clerical <- as.factor(df$occupation == "Adm-clerical")
df1$Sales <- as.factor(df$occupation == "Sales")
df1$Farming_fishing <- as.factor(df$occupation == "Farming-fishing")
df1$Protective_serv <- as.factor(df$occupation == "Protective-serv")
df1$Handlers_cleaners <- as.factor(df$occupation == "Handlers-cleaners")
df1$Armed_Forces <- as.factor(df$occupation == "Armed-Forces")
df1$Priv_house_serv <- as.factor(df$occupation == "Priv-house-serv")

```

Now we don't need occupation.
```{r}
df1$occupation = NULL
```

See correlation again to check.
```{r}
correlation_plot(df1)
```

Both native country and race has very low corelation to income so it make no sense to include them in our analysis.
```{r}
```

Placing our income column to last
```{r}
df1 <- df1[, c(setdiff(names(df1), "income"), "income")]

```

checking column names
```{r}
colnames(df1)
```
Plotting Age distribution
```{r}
ggplot(adult, aes(x = age)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) + 
  labs(title = "Age Distribution", x = "Age", y = "Count") +
  theme_cleveland() +
  theme(plot.title = element_text(hjust = 0.5))  
```

Plotting Work class
```{r}
df1$workclass = as.factor(df1$workclass)

custom_labels_w = c(
  "1" = "Private",  
  "2" = "Goverment",  
  "3" = "Self Employed",  
  "4" = "Without Pay")

ggplot(df1, aes(x = workclass, fill = workclass, group = workclass)) +
  geom_bar(color = "black") +
  labs(title = "WorkClass Distribution", x = "Classes", y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_brewer(palette = "Set2") +
  scale_x_discrete(labels = custom_labels_w)
```

Plotting Education Level
```{r}
df1$education = as.factor(df1$education)

custom_labels_e = c(
  "1" = "Bachelors Level",  
  "2" = "Masters",  
  "3" = "Doctorate",  
  "4" = "Others")

ggplot(df1, aes(x = education, fill = education, group = education)) +
  geom_bar(color = "black") +
  labs(title = "Education Distribution", x = "Classes", y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_brewer(palette = "Set1")+
  scale_x_discrete(labels = custom_labels_e)
  
```

Plotting Marital Status
```{r}
df1$marital.status = as.factor(df1$marital.status)

custom_labels_m = c(
  "1" = "Single", 
  "2" = "Married")

ggplot(df1, aes(x = marital.status, fill = marital.status, group = marital.status)) +
  geom_bar(color = "black") +
  labs(title = "Maritial Status Distribution", x = "Classes", y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_brewer(palette = "Set1") +
   scale_x_discrete(labels = custom_labels_m)
```

Plotting pie chart on gender
```{r}
# Convert the 'sex' column to a factor (if not already)
adult$sex <- as.factor(adult$sex)

# Create a data frame for plotting
gender_df <- as.data.frame(table(adult$sex))
names(gender_df) <- c("Gender", "Count")

# Create a bar graph
ggplot(gender_df, aes(x = Gender, y = Count, fill = Gender)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_fill_manual(values = c("pink", "blue")) +
  labs(title = "Gender Distribution", x = "Gender", y = "Count") +
  geom_text(aes(label = Count), vjust = -0.5, position = position_dodge(0.9), size = 3.5) +
  theme_minimal()

```
```{r}
ggplot(adult, aes(x = income, y = age, fill = income)) + 
  geom_boxplot() +
  labs(title = "Age Distribution by Income", x = "Income", y = "Age")
```


```{r}
ggplot(adult, aes(x = age, y = hours.per.week, color = income)) +
  geom_point(alpha = 0.6) +
  labs(title = "Age vs Hours per Week by Income", x = "Age", y = "Hours per Week")
```


```{r}
ggplot(adult, aes(x = age, y = hours.per.week, size = capital.gain, color = capital.gain)) +
  geom_point(alpha = 0.5) +
  labs(title = "Bubble Chart of Age, Hours Per Week, and Capital Gain", x = "Age", y = "Hours Per Week")
```


```{r}
ggplot(adult, aes(x = education, fill = income)) + 
  geom_bar(position = "fill") +
  labs(title = "Income Distribution Across Education Levels", x = "Education", y = "Proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
ggplot(adult, aes(x = occupation, y = hours.per.week, fill = sex)) + 
  geom_boxplot() +
  labs(title = "Hours Per Week by Occupation and Gender", x = "Occupation", y = "Hours Per Week") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
ggplot(adult, aes(x = age, fill = marital.status)) + 
  geom_density(alpha = 0.7) +
  labs(title = "Age Distribution by Marital Status", x = "Age", y = "Density")
```


```{r}
ggplot(adult, aes(x = hours.per.week)) +
  geom_density(fill = "blue", alpha = 0.5) +
  geom_rug() +
  labs(title = "Density and Rug Plot for Hours Per Week", x = "Hours Per Week", y = "Density")

```
```{r}
marital_income_table <- table(adult$relationship, adult$income)
mosaicplot(marital_income_table, shade = TRUE,
       main = "Mosaic Plot of Marital Status and Income",
       xlab = "Income", ylab = "Marital Status")
```


```{r}
ggplot(adult, aes(x = hours.per.week)) +
  geom_density(fill = "blue", alpha = 0.5) +
  geom_rug() +
  labs(title = "Density and Rug Plot for Hours Per Week", x = "Hours Per Week", y = "Density")
```


```{r}
ggplot(adult, aes(x = age, y = hours.per.week, size = capital.gain, color = capital.gain)) +
  geom_point(alpha = 0.5) +
  labs(title = "Bubble Chart of Age, Hours Per Week, and Capital Gain", x = "Age", y = "Hours Per Week")
```


##Lets start building models

Lets first convert numeric data to factor.

```{r}
df1$workclass = as.factor(df1$workclass)
df1$education = as.factor(df1$education)
df1$marital.status = as.factor(df1$marital.status)
df1$income= as.factor(df1$income)
df1 = na.omit(df1)
```

As we can see every possible numerical data is converted into categories.
```{r}
summary(df1)
```
Creating a loop for each model to run multiple times with random trainnig and testing dataset and getting best accuracy possible.

```{r}
# Initialize an empty data frame to store results
results_df1 <- data.frame(
  Dataset_Number = integer(),
  Model = character(),
  Train_Accuracy = numeric(),
  Test_Accuracy = numeric()
)

# Define a list of models
models <- list(
  Decision_Tree = rpart,
  Random_Forest = randomForest,
  Support_Vector = svm,
  Naive_Bayes = naiveBayes
)

# Create a list to store datasets
dataset_list <- list()

# Loop through models
for (model_name in names(models)) {
  model <- models[[model_name]]
  
  for (i in 1:2) {
    # Split the data
    data = split_show(df1, 0.6)
    df_train = data[[2]]
    df_test = data[[3]]
    
    # Store the dataset in the dataset_list
    dataset_list[[i]] <- list(df_train = df_train, df_test = df_test)
    
    # Fit the model
    if (model_name == "Random_Forest") {
      model <- randomForest(income ~ ., data = df_train, ntree = 5)
    } else if (model_name == "Support_Vector") {
      model <- svm(income ~ ., data = df_train, kernel = "linear")
    } else if (model_name == "Naive_Bayes") {
      model <- naiveBayes(income ~ ., data = df_train)
    } else {
      model <- rpart(income ~ ., data = df_train, method = 'class', cp = -1)
    }
    
    # Make predictions on the train and test data to compare 
    y_pred_train <- predict(model, newdata = df_train, type = 'class')
    y_pred_test <- predict(model, newdata = df_test, type = 'class')
    
    # Calculate train and test accuracy
    train_accuracy <- sum(y_pred_train == df_train$income) / nrow(df_train)
    test_accuracy <- sum(y_pred_test == df_test$income) / nrow(df_test)
    
    # Add results to results_df
    results_df1 <- rbind(results_df1, data.frame(
      Dataset_Number = i,
      Model = model_name,
      Train_Accuracy = train_accuracy,
      Test_Accuracy = test_accuracy
    ))
  }
}

# Display the results
#print(results_df1)


```


Running different loop for logistic Regression because we can't do it in upper loop.

Because other algorithms use "Class" type for prediction and Logistic Regression uses Response type. Its difficult to add logistic in upper loop.
```{r}
# Initialize an empty data frame to store results
results_df_logistic <- data.frame(
  Dataset_Number = integer(),
  Train_Accuracy = numeric(),
  Test_Accuracy = numeric()
)

# Create a list to store datasets
dataset_list_logistic <- list()

#Initializing loop
for (i in 1:2) {
  # Split the data
  data = split_show(df1, 0.6)
  df_train = data[[2]]
  df_test = data[[3]]
  
  # Store the dataset in the dataset_list
  dataset_list_logistic[[i]] <- list(df_train = df_train, df_test = df_test)
  
  # Fit the logistic regression model
  logistic_model <- glm(income ~ ., family = binomial, data = df_train)
  y_pred_train <- predict(logistic_model, newdata = df_train, type = "response")
  y_pred_test <- predict(logistic_model, newdata = df_test, type = "response")
  
  threshold <- 0.5
  y_pred_train_class <- ifelse(y_pred_train >= threshold, 2, 1)
  y_pred_test_class <- ifelse(y_pred_test >= threshold, 2, 1)
  
  # Calculate train and test accuracy
  train_accuracy <- sum(y_pred_train_class == df_train$income) / nrow(df_train)
  test_accuracy <- sum(y_pred_test_class == df_test$income) / nrow(df_test)
  
  # Add results to results_df_logistic
  results_df_logistic <- rbind(results_df_logistic, data.frame(
    Dataset_Number = i,
    Model = "Logistic_Regression",
    Train_Accuracy = train_accuracy,
    Test_Accuracy = test_accuracy
  ))
}

# Display the results
#print(results_df_logistic)
```

Adding logistic Regression to other models and creating one final data frame of train and test accuracy.
```{r}
results_df_final1 <- rbind(results_df1,results_df_logistic)
```

```{r}
#Display the results
results_df_final1
```


```{r}
# Extract the row with the maximum test accuracy
results_df_final1[which.max(results_df_final1$Test_Accuracy), ]

```

Plotting Max test accuracy from each model to compare it with other models.
```{r}
max_test_accuracy <- aggregate(Test_Accuracy ~ Model, data = results_df_final1, FUN = max)

bar_colors <- c("lightblue", "firebrick", "lightyellow", "darkblue", "seagreen")

# Create a bar plot with different colors for each bar
ggplot(data = max_test_accuracy, aes(x = Model, y = Test_Accuracy, fill = Model)) +
  geom_bar(stat = "identity", color = "black") +  # Set the outline color to black
  scale_fill_manual(values = bar_colors) +  # Assign custom colors to bars
  geom_text(aes(label = round(Test_Accuracy, 2)), vjust = -0.5, size = 3.5) +  # Add text labels
  labs(title = "Maximum Test Accuracy for Each Model",
       x = "Model",
       y = "Max Test Accuracy") +
  theme_minimal() +
  theme(legend.title = element_blank())
```

Scatter plot of the same thing
```{r}
ggplot(data = results_df_final1, aes(x = Train_Accuracy, y = Test_Accuracy)) +
  geom_point() +
  labs(title = "Scatter Plot of Train Accuracy vs. Test Accuracy",
       x = "Train Accuracy",
       y = "Test Accuracy") +
  theme_minimal()
```
```{r}
df1 = na.omit(df1)
```

```{r}
df2 = df1
```


```{r}
 fit=randomForest(factor(income)~., data=df1)
```


```{r}
varImpPlot(fit,type=2)
```



```{r}
#df2$Exec_managerial = NULL
#df2$Prof_specialty= NULL
df2$Machine_op_inspct= NULL
df2$Transport_moving= NULL
df2$Craft_repair= NULL
df2$Handlers_cleaners= NULL
df2$Farming_fishing= NULL
df2$Adm_clerical= NULL
df2$Other_service= NULL
df2$Adm_clerical = NULL
df2$Sales =NULL
df2$Farming_fishing = NULL
df2$Protective_serv = NULL
df2$Handlers_cleaners = NULL
df2$Armed_Forces = NULL
df2$Priv_house_serv = NULL
df2$Tech_support = NULL
df2$fnlwgt = NULL
#df2$education.num = NULL
df2$race = NULL
df2$native.country = NULL

```



In this we deleted all the columns from occupation which we made earlier, fnlwgt, education number and race.









```{r}
knn_class_st = train(income ~ ., data = df_train, method = "knn",tuneGrid = data.frame(k = seq(10)))
```


```{r}
plot(knn_class_st)
```


```{r}
xgbTree_classifier = train(income ~ ., data = df_train,method = 'xgbTree',verbose = 1)
```


```{r}
plot(xgbTree_classifier)
```

```{r}
df2 = na.omit(df2)

df2$income = as.factor(df2$income)
```


```{r}
results_df2 <- data.frame(
  Dataset_Number = integer(),
  Model = character(),
  Train_Accuracy = numeric(),
  Test_Accuracy = numeric()
)

# Define a list of models
models <- list(
  Decision_Tree = rpart,
  Random_Forest = randomForest,
  Support_Vector = svm,
  Naive_Bayes = naiveBayes
)

# Create a list to store datasets
dataset_list2 <- list()

# Loop through models
for (model_name in names(models)) {
  model <- models[[model_name]]
  
  for (i in 1:2) {
    # Split the data
    data = split_show(df2, 0.6)
    df_train = data[[2]]
    df_test = data[[3]]
    
    # Store the dataset in the dataset_list
    dataset_list2[[i]] <- list(df_train = df_train, df_test = df_test)
    
    # Fit the model
    if (model_name == "Random_Forest") {
      model <- randomForest(income ~ ., data = df_train, ntree = 5)
    } else if (model_name == "Support_Vector") {
      model <- svm(income ~ ., data = df_train, kernel = "linear")
    } else if (model_name == "Naive_Bayes") {
      model <- naiveBayes(income ~ ., data = df_train)
    }else {
      model <- rpart(income ~ ., data = df_train, method = 'class', cp = -1)
    }
    
    # Make predictions on the test data
    y_pred_train <- predict(model, newdata = df_train, type = 'class')
    y_pred_test <- predict(model, newdata = df_test, type = 'class')
    
    # Calculate train and test accuracy
    train_accuracy <- sum(y_pred_train == df_train$income) / nrow(df_train)
    test_accuracy <- sum(y_pred_test == df_test$income) / nrow(df_test)
    
    # Add results to results_df
    results_df2 <- rbind(results_df2, data.frame(
      Dataset_Number = i,
      Model = model_name,
      Train_Accuracy = train_accuracy,
      Test_Accuracy = test_accuracy
    ))
  }
}
```


```{r}
results_df2_extra <- data.frame(
  Dataset_Number = integer(),
  Model = character(),
  Train_Accuracy = numeric(),
  Test_Accuracy = numeric()
)

# Define a list of models
models <- list(
  XGBtree = train,
  KNN = train
)

# Create a list to store datasets
dataset_list2_extra <- list()

# Loop through models
for (model_name in names(models)) {
  model <- models[[model_name]]
  
  for (i in 1:2) {
    # Split the data
    data = split_show(df2, 0.6)
    df_train = data[[2]]
    df_test = data[[3]]
    
    # Store the dataset in the dataset_list
    dataset_list2_extra[[i]] <- list(df_train = df_train, df_test = df_test)
     if (model_name == "XGBtree"){
      model = train(income ~ ., data = df_train,method = 'xgbTree',verbose = 1)
    } else {
      model = train(income ~ ., data = df_train, method = "knn")#tuneGrid = data.frame(k = seq(10)))
    }
    
    y_pred_train <- predict(model, newdata = df_train, type = 'raw')
    y_pred_test <- predict(model, newdata = df_test, type = 'raw')
    
    # Calculate train and test accuracy
    train_accuracy <- sum(y_pred_train == df_train$income) / nrow(df_train)
    test_accuracy <- sum(y_pred_test == df_test$income) / nrow(df_test)
    
    # Add results to results_df
    results_df2_extra <- rbind(results_df2_extra, data.frame(
      Dataset_Number = i,
      Model = model_name,
      Train_Accuracy = train_accuracy,
      Test_Accuracy = test_accuracy
    ))
  }
}
    
```
```{r}
results_df_logistic2 <- data.frame(
  Dataset_Number = integer(),
  Train_Accuracy = numeric(),
  Test_Accuracy = numeric()
)

# Create a list to store datasets
dataset_list_logistic2 <- list()

for (i in 1:2) {
  # Split the data
  data = split_show(df2, 0.6)
  df_train = data[[2]]
  df_test = data[[3]]
  
  # Store the dataset in the dataset_list
  dataset_list_logistic2[[i]] <- list(df_train = df_train, df_test = df_test)
  
  # Fit the logistic regression model
  logistic_model <- glm(income ~ ., family = binomial, data = df_train)
  y_pred_train <- predict(logistic_model, newdata = df_train, type = "response")
  y_pred_test <- predict(logistic_model, newdata = df_test, type = "response")
  
  threshold <- 0.5
  y_pred_train_class <- ifelse(y_pred_train >= threshold, 2, 1)
  y_pred_test_class <- ifelse(y_pred_test >= threshold, 2, 1)
  
  # Calculate train and test accuracy
  train_accuracy <- sum(y_pred_train_class == df_train$income) / nrow(df_train)
  test_accuracy <- sum(y_pred_test_class == df_test$income) / nrow(df_test)
  
  # Add results to results_df_logistic
  results_df_logistic2 <- rbind(results_df_logistic2, data.frame(
    Dataset_Number = i,
    Model = "Logistic_Regression",
    Train_Accuracy = train_accuracy,
    Test_Accuracy = test_accuracy
  ))
}
```
```{r}
results_df_final2 <- rbind(results_df2,results_df_logistic2,results_df2_extra)
results_df_final2[which.max(results_df_final2$Test_Accuracy), ]
```


```{r}
results_df_final2
```


```{r}
max_test_accuracy <- aggregate(Test_Accuracy ~ Model, data = results_df_final2, FUN = max)

bar_colors <- c("lightblue", "firebrick", "black", "darkblue", "seagreen","purple","pink")

# Create a bar plot with different colors for each bar
ggplot(data = max_test_accuracy, aes(x = Model, y = Test_Accuracy, fill = Model)) +
  geom_bar(stat = "identity", color = "black") +  # Set the outline color to black
  scale_fill_manual(values = bar_colors) +  # Assign custom colors to bars
  geom_text(aes(label = round(Test_Accuracy, 2)), vjust = -0.3, size = 3) +  # Add text labels
  labs(title = "Maximum Test Accuracy for Each Model",
       x = "Model",
       y = "Max Test Accuracy") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}

# Create a line chart with different colors for each line
ggplot(data = max_test_accuracy, aes(x = Model, y = Test_Accuracy, group = Model, color = Model)) +
  geom_line() +  # Use geom_line to create a line chart
  geom_point(size = 3) +  # Add points for data points
  scale_color_manual(values = bar_colors) +  # Assign custom colors to lines
  geom_text(aes(label = round(Test_Accuracy, 2)), vjust = -0.9, size = 4) +  # Add text labels
  labs(title = "Maximum Test Accuracy for Each Model",
       x = "Model",
       y = "Max Test Accuracy") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0.75, 0.90)

```


```{r}
df_test$income = as.numeric(df_test$income)
roc_obj = roc(df_test$income, y_pred_test)
```

```{r}
plot(roc_obj, main="ROC Curve", col="#1c61b6", lwd=4)
abline(a=0, b=1, lty=2, col="red") 
```

```{r}
auc(roc_obj)
```
```{r}

```


```{r}
# Replace "Your_Test_Data" with your actual test data
predictions <- predict(tree_model, newdata = df_test)

```


```{r}
tree_model = rpart(income ~., data = df_train, 
  method = 'class', #method for classification trees. 
  parms = list(split = 'information'), 
  minsplit = 2, 
  minbucket = 1, #Run model without the last two to get a full tree.
  maxdepth = 10,
  cp = -1)
```


```{r}
y_pred_train = predict(tree_model,newdata=df_train,type = 'class' )
y_pred_test = predict(tree_model,newdata=df_test, type = 'class')

cat('TRAIN RESULTS \n\n')

df_train_results = data.frame(y_pred_train,df_train$income)
confusionMatrix(table(df_train_results))

cat('\n\nTEST RESULTS\n\n')

df_test_results = data.frame(y_pred_test,df_test$income)
confusionMatrix(table(df_test_results))
```


```{r}

```
```{r}
library(randomForest)
```


```{r}
plot(rf_model)
```


```{r}
rf_model <- randomForest(income ~ ., data = df_train, ntree = 5)
```


```{r}
y_pred_train = predict(rf_model,newdata=df_train,type = 'class' )
y_pred_test = predict(rf_model,newdata=df_test, type = 'class')

cat('TRAIN RESULTS \n\n')

df_train_results = data.frame(y_pred_train,df_train$income)
confusionMatrix(table(df_train_results))

cat('\n\nTEST RESULTS\n\n')

df_test_results = data.frame(y_pred_test,df_test$income)
confusionMatrix(table(df_test_results))
```
```{r}
svm_model <- svm(income ~ ., data = df_train, kernel = "linear")

nb_model <- naiveBayes(income ~ ., data = df_train)
```


```{r}
y_pred_train = predict(svm_model,newdata=df_train,type = 'class' )
y_pred_test = predict(svm_model,newdata=df_test, type = 'class')

df_test_results = data.frame(y_pred_test,df_test$income)
confusionMatrix(table(df_test_results))
```
```{r}
svm_pred = prediction(y_pred_test, df_test$income)
svm_perf = performance(y_pred_test,'tpr','fpr')
plot(svm_perf, colorize = TRUE)
abline(a = 0, b = 1, col = "gray") 
```


```{r}
y_pred_train = predict(nb_model,newdata=df_train,type = 'class' )
y_pred_test = predict(nb_model,newdata=df_test, type = 'class')

df_test_results = data.frame(y_pred_test,df_test$income)
confusionMatrix(table(df_test_results))
```


```{r}
logistic_model <- glm(income ~ ., family = binomial, data = df_train)
```


```{r}
y_pred_train = predict(logistic_model,newdata=df_train,type = 'response' )
y_pred_test = predict(logistic_model,newdata=df_test, type = 'response')

```

```{r}
library(forestplot)
odds_ratios <- exp(coef(logistic_model))
forestplot(odds_ratios, main = "Odds Ratios", is.summary = "response")
```

```{r}
library(effects)
effect_plot <- effect('income', logistic_model)
plot(effect_plot, main = "Effects Plot")
```
```{r}
residuals <- residuals(logistic_model, type = "response")
predicted_probs <- predict(logistic_model, type = "response")
plot(predicted_probs, residuals, ylab = "Residuals", xlab = "Predicted Probabilities")
abline(h = 0, col = "red")
```





```{r}

xgb_model = train(income ~ ., data = df_train,method = 'xgbTree',verbose = 1)

knn_model = train(income ~ ., data = df_train, method = "knn")

```
```{r}
# Create an explainer for the XGBoost model
explainer <- createExplainer(xgb_model)

# Plot the first tree
plot_tree(explainer, tree_number = 0)

```


```{r}
y_pred_train = predict(xgb_model,newdata=df_train,type = 'raw' )
y_pred_test = predict(xgb_model,newdata=df_test, type = 'raw')


df_test_results = data.frame(y_pred_test,df_test$income)
confusionMatrix(table(df_test_results))
```


```{r}
y_pred_train = predict(knn_model,newdata=df_train,type = 'raw' )
y_pred_test = predict(knn_model,newdata=df_test, type = 'raw')


df_test_results = data.frame(y_pred_test,df_test$income)
confusionMatrix(table(df_test_results))
```

